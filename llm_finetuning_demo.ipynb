{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model (LLM) Fine-tuning Demonstration\n",
    "\n",
    "## Purpose:\n",
    "This notebook serves as a demonstration of fine-tuning Large Language Models (LLMs) on task-specific instructions to enhance their performance on specialized tasks. The process of fine-tuning adjusts the parameters of a pre-trained model to make it more suited to a specific task, allowing it to generate more accurate and coherent responses.\n",
    "\n",
    "## Model:\n",
    "In this demonstration, we are using the `opt-350M` model, an opensource model from Meta. This model has been pre-trained on a diverse and extensive corpus, enabling it to understand and generate human-like text based on the given prompts or instructions.\n",
    "\n",
    "## Dataset:\n",
    "We are utilizing the `samsum` dataset, a collection of dialogues and their corresponding summaries. This dataset is ideal for training models on the task of dialogue summarization, allowing them to learn how to generate concise and informative summaries of conversations.\n",
    "\n",
    "## Task:\n",
    "The primary task in this notebook is **Dialog Summarization**. The model will be fine-tuned to summarize dialogues, generating brief and coherent summaries that retain the essential information from the conversations.\n",
    "\n",
    "## Process:\n",
    "1. **Pre-fine-tuning Performance Assessment:**\n",
    "   - We will assess the model's ability to perform dialog summarization before fine-tuning, using samples from the `samsum` dataset.\n",
    "   \n",
    "2. **Fine-tuning:**\n",
    "   - The `opt-350M` model will be fine-tuned on the `samsum` dataset, learning to generate accurate and concise summaries of dialogues.\n",
    "   \n",
    "3. **Post-fine-tuning Performance Assessment:**\n",
    "   - We will evaluate the model's performance on dialog summarization after fine-tuning, comparing it with the pre-fine-tuning performance to observe the improvements.\n",
    "\n",
    "## Objective:\n",
    "The objective of this notebook is to showcase the effectiveness of fine-tuning LLMs on task-specific instructions, emphasizing how it can significantly enhance the model's performance on specialized tasks like dialog summarization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Language Modeling (CLM) Pretraining Overview\n",
    "\n",
    "Causal Language Modeling is a technique where the model generates text sequentially, predicting the next word in a sequence based on the previous words. It learns to predict the probability \\( P(w_t | w_{1}, w_{2}, ..., w_{t-1}) \\) of a word \\( w_t \\) at time \\( t \\) given the preceding words.\n",
    "\n",
    "### Pretraining Steps:\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - A large corpus of text data is collected from diverse sources.\n",
    "   - The text is tokenized into subwords or words.\n",
    "   \n",
    "2. **Model Initialization:**\n",
    "   - The parameters of a transformer model, typically used for CLM, are initialized.\n",
    "   \n",
    "3. **Masked Language Modeling:**\n",
    "   - During pretraining, a variant called Masked Language Model (MLM) is often used where some input tokens are masked, and the model learns to predict them.\n",
    "   \n",
    "4. **Training:**\n",
    "   - The model is trained to predict the next word in a sequence based on the preceding words.\n",
    "   - It learns contextual representations of words by adjusting its parameters to minimize the difference between the predicted probability distribution and the true distribution of the next word.\n",
    "   \n",
    "5. **Evaluation and Fine-tuning:**\n",
    "   - The pretrained model is evaluated on specific tasks and fine-tuned on task-specific datasets.\n",
    "\n",
    "### Diagrammatic Representation:\n",
    "\n",
    "The diagram below represents a simple sequence and the causal relationships between the words in the sequence.\n",
    "\n",
    "\n",
    "<img src=\"images/Pretraining.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Tuning Overview\n",
    "\n",
    "Instruction tuning is a crucial step in the deployment of Large Language Models (LLMs) like GPT-3.5 Turbo. It involves fine-tuning the model to understand and generate responses to specific instructions or prompts, enhancing its performance on various tasks.\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "1. **Selection of Prompts:**\n",
    "   - Choose a set of prompts or instructions that are representative of the task you want the model to perform.\n",
    "   \n",
    "2. **Creation of Training Dataset:**\n",
    "   - Construct a dataset consisting of the selected prompts along with the corresponding correct responses.\n",
    "   \n",
    "3. **Fine-tuning:**\n",
    "   - The model is fine-tuned on the training dataset, learning to generate responses that are coherent and contextually relevant to the given prompts.\n",
    "   \n",
    "4. **Evaluation:**\n",
    "   - After fine-tuning, the model is evaluated on a separate dataset to assess its performance and make any necessary adjustments.\n",
    "\n",
    "### Importance:\n",
    "\n",
    "- **Enhanced Performance:**\n",
    "   - Instruction tuning allows the model to generate more accurate and coherent responses, improving its overall performance on specific tasks.\n",
    "   \n",
    "- **Task Specificity:**\n",
    "   - It enables the model to understand and respond to task-specific instructions, making it more versatile and applicable to a range of use cases.\n",
    "\n",
    "### Illustration:\n",
    "\n",
    "The Python code below illustrates a simplistic example of instruction tuning, where a hypothetical model is fine-tuned to respond to a specific instruction.\n",
    "\n",
    "<img src=\"images/fine-tuning-task-specific.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task specific examples\n",
    "\n",
    "\n",
    "<img src=\"images/fine-tuning-task-specific-example.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Start Notebook\n",
    "\n",
    "This notebook shows how to train a OPT-350M model on a single GPU (e.g. A100 with 80GB) using int8 quantization and LoRA.\n",
    "\n",
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "The example uses the Hugging Face trainer and model. We will download model and data both from hugging face.\n",
    "\n",
    "Let's proceed with the demonstration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#!pip3 install transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES environment variable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-350m\",\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Summarize this dialog:\n",
    "Person A: Hey, are you free this weekend?\n",
    "Person B: Hi! I might have some time on Saturday. Why do you ask?\n",
    "Person A: I was thinking of organizing a small get-together at my place. Just a few close friends. Would you like to come?\n",
    "Person B: That sounds like a lot of fun! Iâ€™d love to come. What time are you thinking?\n",
    "Person A: I was thinking around 7 PM. We can have some snacks and drinks.\n",
    "Person B: 7 PM works for me. Should I bring something?\n",
    "Person A: If you could bring some drinks, that would be great!\n",
    "Person B: Sure, I can do that. Looking forward to it!\n",
    "Person A: Awesome! See you on Saturday then!\n",
    "---\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# \"\"\"How to train a dog to sit?\"\"\"\n",
    "\n",
    "# model = model_350\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 3: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config =  LoraConfig(\n",
    "                                r=16,\n",
    "                                lora_alpha=32,\n",
    "                                target_modules=[\"q_proj\", \"v_proj\"],\n",
    "                                lora_dropout=0.05,\n",
    "                                bias=\"none\",\n",
    "                                task_type=\"CAUSAL_LM\"\n",
    "                            )\n",
    "\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    # model = prepare_model_for_int8_training(model)\n",
    "    # model.print_trainable_parameters()\n",
    "    total_parameters = sum(p.numel() for p in model.parameters())\n",
    "    print(\"total number of parameters in model : \", total_parameters )\n",
    "    model = get_peft_model(model, peft_config , adapter_name = \"A1\")\n",
    "    # model = get_peft_model(model, peft_config , adapter_name = \"A2\")\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load the pre-processed dataset and fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator , DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from itertools import chain\n",
    "\n",
    "class Concatenator(object):\n",
    "    def __init__(self, chunk_size=1024):\n",
    "        self.chunk_size=chunk_size\n",
    "        self.residual = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        concatenated_samples = {\n",
    "            k: v + list(chain(*batch[k])) for k, v in self.residual.items()\n",
    "        }\n",
    "\n",
    "        total_length = len(concatenated_samples[list(concatenated_samples.keys())[0]])\n",
    "\n",
    "        if total_length >= self.chunk_size:\n",
    "            chunk_num = total_length // self.chunk_size\n",
    "            result = {\n",
    "                k: [\n",
    "                    v[i : i + self.chunk_size]\n",
    "                    for i in range(0, chunk_num * self.chunk_size, self.chunk_size)\n",
    "                ]\n",
    "                for k, v in concatenated_samples.items()\n",
    "            }\n",
    "            self.residual = {\n",
    "                k: v[(chunk_num * self.chunk_size) :]\n",
    "                for k, v in concatenated_samples.items()\n",
    "            }\n",
    "        else:\n",
    "            result = concatenated_samples\n",
    "            self.residual = {k: [] for k in concatenated_samples.keys()}\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def get_preprocessed_samsum(tokenizer, split):\n",
    "    dataset = load_dataset(\"samsum\", split=split)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Summarize this dialog:\\n{{dialog}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "    )\n",
    "\n",
    "    def apply_prompt_template(sample):\n",
    "        return {\n",
    "            \"text\": prompt.format(\n",
    "                dialog=sample[\"dialogue\"],\n",
    "                summary=sample[\"summary\"],\n",
    "                eos_token=tokenizer.eos_token,\n",
    "            )\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(apply_prompt_template, remove_columns=list(dataset.features))\n",
    "        \n",
    "    dataset = dataset.map(\n",
    "        lambda sample: tokenizer(sample[\"text\"]),\n",
    "        batched=True,\n",
    "        remove_columns=list(dataset.features),\n",
    "    ).map(Concatenator(), batched=True)\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "data3 = get_preprocessed_samsum(tokenizer, 'train')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data3,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=200,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/glb/data/gw_export/bootcamp/<your_id>/output'\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200)[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
